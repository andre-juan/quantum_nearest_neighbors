{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Nearest Neighbors\n",
    "\n",
    "\n",
    "In this [repository](https://github.com/andre-juan/quantum_nearest_neighbors), I implement the algorithm introduced in [this paper](https://arxiv.org/pdf/1412.3646.pdf) by [Maria Schuld](https://scholar.google.com/citations?user=_ih_hwUAAAAJ&hl=en), [Ilya Sinayskiy](https://scholar.google.co.za/citations?user=tL1_WfsAAAAJ&hl=en) and [Francesco Petruccione](https://scholar.google.com/citations?user=chM4fT4AAAAJ&hl=en)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The algorithm is proposed to solve **classification problems**: assign one out of a number of discrete classes to an observation, according to a rule learned from a set of labeled (classified) examples. Some examples of such problems are:\n",
    "\n",
    "- Diagnosing a disease given a number of symptoms;\n",
    "- Credit decision based on probability of default;\n",
    "- An email being automatically marked as \\'important\\' or \\'spam\\';\n",
    "- A handwritten digit on a postal envelope being recognised by a scanning device. \n",
    "\n",
    "We will be working in the **supervised learning** paradigm:\n",
    "\n",
    "- We have a set of $n$-dimensional data vectors $\\{\\vec{v}^p\\}$;\n",
    "  \n",
    "- And their respective class assignments $c^p$. Classes are often encoded by a finite number $d$ of positive integers $c\\in\\{1,...,d\\}$. A particular case of interest is that of **binary classification**, in which case $d=2$, and $c\\in\\{0, 1\\}$;\n",
    "  \n",
    "- $\\mathcal{T}=\\{ (\\vec{v}^{p}, c^p) \\}_{p=1,\\cdots, N}$ makes up the training set.\n",
    "  \n",
    "  \n",
    "Each one of the training vectors encodes $n$ *features* (or *predictors*) $v_{i}^p$, $i=1,\\cdots, n$.  Features may be given by binary, integer or real-valued numbers.\n",
    "  \n",
    " \n",
    "In the supervised learning paradign, we can schematically see a classification problem as:\n",
    "\n",
    "> Given: \n",
    "> - Training set $\\mathcal{T}=\\{ (\\vec{v}^{p}, c^p) \\}_{p=1,\\cdots, N}$;\n",
    "> - Unclassified vector $\\vec{x}$, encoding $n$ features. \n",
    "\n",
    "  \n",
    "> Goal: \n",
    "> - Match the input vector $\\vec{x}$ to a class, using information from the training data. \n",
    "\n",
    "  Possible approach for classification:\n",
    "\n",
    "- Define some distance measure;\n",
    "- Assign the input vector to the class whose members are most similar in terms of this distance (i.e., whose members are closest to it). \n",
    "\n",
    "A common distance measure is the Hamming distance, which is used when the features are encoded as **binary strings**. It is defined as the number of different bits, or components, among a pair of binary strings.\n",
    "\n",
    "> Ex.: Take $a = 1011101$ and $b = 1001001$. $d_H(a,b) = 2$.\n",
    "\n",
    "______________\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **quantum nearest neighbors** algorithm is a quantum classification algorithm, which uses the aforementioned distance criterion for classification, based on Hamming distance. Schematically, the algorithm goes like:\n",
    "  \n",
    "- We first create a superposition of the training data set;\n",
    "  \n",
    "- Then write the Hamming distance between the input observation and each example in the training set into the amplitude of each vector in the superposition;\n",
    "  \n",
    "- Measuring the class-qudit then retrieves the desired class with the highest probability. \n",
    "\n",
    "Step by step, we have:\n",
    "\n",
    "___________________\n",
    "\n",
    "\n",
    "**STEP 0 - state preparation**\n",
    "\n",
    "- First, extract the features of the datapoints in the training set and store them as bit strings/bit vectors. This is done classically, via some preprocessing routine.\n",
    "\n",
    "- Then map these bit vectors to quantum computational-basis states: $0 \\mapsto \\left | 0 \\right >$ and $1 \\mapsto \\left | 1 \\right >$.\n",
    "\n",
    "After these two steps, each training set datapoint is mapped to the quantum state $\\left | v_1^p \\cdots v_n^p \\right > \\equiv \\left | \\vec{v}^p \\right >$, $v_k^p \\in \\{0,1\\} $, $p = 1, \\cdots, N$.\n",
    "\n",
    "Consider also the class $c^p \\in \\{1, \\cdots, d\\}$, and construct the state:\n",
    "  \n",
    "$\\left | v_1^p \\cdots v_n^p, c^p \\right > \\equiv \\left |\\vec{v}^p, c^p\\right > \\in \\mathcal{H}_{2}^{\\otimes n} \\otimes \\mathcal{H}_d \\$.\n",
    "\n",
    "If we're dealing with binary classification, the classes are also straightforwardly encoded in qubits as $0 \\mapsto \\left | 0 \\right >$ and $1 \\mapsto \\left | 1 \\right >$. If we have a multiclass problem, qudits are necessary, ore one could use qubits to encode integers corresponding to the classes (for instance, $\\left | 011 \\right > = \\left | 3 \\right >$) \n",
    "\n",
    "Once we have the training states  $\\{\\left |\\vec{v}^p, c^p\\right > \\}\\in \\mathcal{H}_{2}^{\\otimes n} \\otimes \\mathcal{H}_d$, $p = 1,...,N$, we construct a training set superposition of all datapoints,\n",
    "  \n",
    "$\\left |T\\right > = \\frac{1}{\\sqrt{N}} \\sum_{p=1}^N \\left |\\vec{v}^p, c^p\\right >.$\n",
    "  \n",
    "Now, do the same binarization process with the unclassified n-dimensional input vector $\\vec{x}$, and map it to the feature vector $ \\left |x_1 \\cdots x_n\\right > \\equiv \\left |\\vec{x}\\right >$, $x_k \\in \\{0,1\\}$.\n",
    "\n",
    "Finally, add an ancilla register $\\left | 0 \\right >$ as the last register.\n",
    "\n",
    "From this, we construct the initial state:\n",
    "  \n",
    "$\\left |\\psi_0\\right > = \\frac{1}{\\sqrt{N}} \\sum_{p=1}^N \\left |\\vec{x};\\vec{v}^p, c^p ; 0\\right > \\ ,$\n",
    " \n",
    "which is made of three registers: \n",
    "\n",
    "- The first containing the input state $\\left |\\vec{x}\\right >$;\n",
    "- The second  containing the superposition $\\left |T\\right >$ (which is the tensor product of the feature vectors $\\left |\\vec{v}^p\\right >$ and the class vectors $\\left |c^p\\right >$); \n",
    "- The third containing an ancilla qubit set to $\\left |0\\right >$. \n",
    "\n",
    "Once the initial state is prepared, the proper operations of the algorithm begin!\n",
    "\n",
    "___________________\n",
    "\n",
    "\n",
    "**STEP 1**\n",
    "\n",
    "The ancilla is put into a superposition,\n",
    "  \n",
    "$ \\left |\\psi_1\\right > = \\frac{1}{\\sqrt{N}} \\sum_{p=1}^N \\left |\\vec{x};\\vec{v}^p, c^p \\right > \\otimes \\frac{1}{\\sqrt{2}} (\\left |0\\right >+ \\left |1\\right >). $\n",
    "\n",
    "___________________\n",
    "\n",
    "\n",
    "**STEP 2**\n",
    "\n",
    "The Hamming distance between each qubit of the first (input) and second (training) register replaces the qubits in the second register, \n",
    "  \n",
    "$ d_k^{i} = \\left\\{   \\begin{array}{l l}\n",
    "\t\t      0, & \\quad \\mathrm{if} \\; \\left |v_k^p\\right > = \\left |x_k\\right >,\\\\\n",
    "\t\t      1, & \\quad \\mathrm{else,} \n",
    "\t\t  \\end{array} \\right . $ \n",
    "\t\n",
    "\n",
    "To do this, apply a $\\mathrm{cNOT}(x_k,v_k^p)$-gate, which overwrites the second entry $v_k^p$ with 0 if $v_k^p=x_k$ and else with $1$:\n",
    "   \n",
    "$ \\left\\{\\begin{matrix}\n",
    "        \\mathrm{cNOT} \\left |00\\right > = \\left |00\\right > \\ ; \\  \\mathrm{cNOT} \\left |01\\right > = \\left |01\\right >\\\\ \n",
    "        \\mathrm{cNOT} \\left |11\\right > = \\left |10\\right > \\ ; \\ \\mathrm{cNOT} \\left |10\\right > = \\left |11\\right >\n",
    "        \\end{matrix}\\right. $\n",
    "\n",
    "Thus, the second step accounts to\n",
    "  \n",
    "$\\left |\\psi_2\\right > = \\prod_{k=1}^n \\mathrm{cNOT}(x_k, v^p_k) \\; \\left |\\psi_1\\right > = \\frac{1}{\\sqrt{N}}\\sum_{p=1}^N \\left |\\vec{x};\\vec{d}^p, c^p\\right > \\otimes \\frac{1}{\\sqrt{2}}(\\left |0\\right > + \\left |1\\right >)\n",
    "  ,$\n",
    "  \n",
    "where the Hamming distance components $\\left |d_1^p \\cdots d_n^p\\right > \\equiv \\left |\\vec{d}^p\\right >$, $d_k^p \\in \\{0,1\\}$, $p = 1, \\cdots, N$ are now in the second register.\n",
    "\n",
    "___________________\n",
    "\n",
    "\n",
    "**STEP 3**\n",
    "\n",
    "Use the unitary operator\n",
    "\n",
    "$U = e^{ -i\\frac{\\pi }{2n} H}  \\ ; \\qquad\\; H = 1 \\otimes \\sum_{k=1}^n \\left(\\frac{1-\\sigma_z}{2}\\right)_{d_k}  \\otimes 1 \\otimes \\sigma_z\\; , $\n",
    "\n",
    "This sums the Hamming distance components $\\{d^p_k\\}$ (thus yielding the actual Hamming distance) between $\\left |\\vec{v}^p\\right >$ and $\\left |\\vec{x}\\right >$, $d_H(\\vec{x},\\vec{v}^p) \\equiv d_H$, into the phase of the $p$th state of the superposition.\n",
    "\n",
    "A negative sign is added, depending on the ancilla state. \n",
    "\n",
    "The state after the third step is:\n",
    "\n",
    "$\\left |\\psi_3\\right > = U \\left |\\psi_2\\right > = \\frac{1}{\\sqrt{2N}} \\sum_{p=1}^N  \\left ( e^{ -i\\frac{\\pi }{2n} d_H}  \\left |\\vec{x};\\vec{d}^p, c^p; 0\\right > + e^{i\\frac{\\pi }{2n} d_H} \\left |\\vec{x};\\vec{d}^p, c^p; 1\\right > \\right )$\n",
    "\n",
    "___________________\n",
    "\n",
    "**STEP 4**\n",
    "\n",
    "Apply another Hadamard on the ancilla, $H = 1 \\otimes 1 \\otimes 1 \\otimes H$,\n",
    "\n",
    "$\\left |\\psi_4\\right > = H \\left |\\psi_3\\right > = \\frac{1}{\\sqrt{N}} \\sum_{p=1}^N  \\left ( \\cos\\left (\\frac{\\pi d_H}{2n}\\right) \\left |\\vec{x};\\vec{d}^p, c^p;0\\right > + \\sin\\left (\\frac{\\pi d_H}{2n}\\right) \\left |\\vec{x};\\vec{d}^p, c^p;1\\right > \\right ) $\n",
    "\n",
    "Notice that $0 \\leq d_H  \\leq n \\Rightarrow 0 \\leq \\frac{\\pi d_H}{2n} \\leq \\frac{\\pi}{2}$. Therefore,\n",
    "  \n",
    "\n",
    "- For large $d_H$, $\\cos\\left (\\frac{\\pi d_H}{2n}\\right) \\rightarrow 0$ and $\\sin\\left (\\frac{\\pi d_H}{2n}\\right) \\rightarrow 1$, so that we have higher probability of measuring $\\left |1\\right >$; \n",
    "\n",
    "- For short $d_H$, $\\cos\\left (\\frac{\\pi d_H}{2n}\\right) \\rightarrow 1$ and $\\sin\\left (\\frac{\\pi d_H}{2n}\\right) \\rightarrow 0$, so that we have higher probability of measuring $\\left |0\\right >$;  \n",
    "\n",
    "That is, if the new input is far away from most training observations, we have a much higher probability to measure the ancilla in the state $\\left |1\\right >$, if the input is close to many observations we end up in state $\\left |0\\right >$. \n",
    "\n",
    "___________________\n",
    "\n",
    "**STEP 5 - Measurement**\n",
    "\n",
    "Now measure the ancilla of  $\\left |\\psi_4\\right >$ in the computational basis. The probability of measuring $\\left |0\\right >$ is\n",
    "  \n",
    "    \n",
    "$P(\\left |0\\right >_a) = \\left | \\left < 0 | \\psi_4\\right > \\right |^2 =  \\frac{1}{N} \\sum_{p=1}^N  \\cos^2\\left (\\frac{\\pi d_H}{2n} \\right)$\n",
    " \n",
    "\n",
    "Rewrite $\\left |\\psi_4\\right >$, to show that the different classes appear weighted by their member's distance to the input,\n",
    "  \n",
    "\n",
    "$\\left |\\psi_4\\right > = \\frac{1}{\\sqrt{N}} \\sum \\limits_{c=1}^d \\left |c\\right > \\otimes \\sum \\limits_{l\\in c} \\left ( \\mathrm{cos}\\left( \\frac{\\pi d_H}{2n}\\right)  \\left |\\vec{x};\\vec{d}^l; 0\\right > +  \\mathrm{sin}\\left( \\frac{\\pi d_H}{2n}\\right)  \\left |\\vec{x};\\vec{d}^l; 1\\right > \\right )  , $\n",
    "\n",
    "where $l$  runs over all training vectors classified with the label $c$.\n",
    "\n",
    "The joint conditional probability to measure a certain class $c \\in \\{1,...,d\\}$ provided we previously measured the ancilla in $0$ (state collapsed to $\\left |\\tilde{\\psi}_4\\right > = \\left < 0 | \\psi_4\\right >\\left |0\\right >$) is\n",
    "  \n",
    "$P( c \\mid \\left |0\\right >_a ) = P(c)P(\\left |0\\right >_a) =  \\left |  \\left < c | \\tilde{\\psi}_4 \\right > \\right |^2 =  \\frac{1}{N} \\sum \\limits_{l\\in c} \\cos^2\\left (\\frac{\\pi d_H}{2n} \\right) \\ ,$\n",
    "\n",
    "so that: \n",
    " \n",
    "$P(c) =  \\frac{1}{ P(\\left |0\\right >_a)}\\frac{1}{N} \\sum \\limits_{l\\in c} \\cos^2\\left (\\frac{\\pi d_H}{2n} \\right) \\ .$\n",
    "\n",
    "Thus, the class measured with highest probability is that whose members are the closest to the input vector! \n",
    "\n",
    "And, finally, the most probable class is chosen as the final prediction!\n",
    "\n",
    "Now, head to [the repository](https://github.com/andre-juan/quantum_nearest_neighbors), and you'll find Python/Qiskit implementations of the algorithm described above. Have fun! :D\n",
    "_________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
